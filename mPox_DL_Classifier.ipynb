{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heyronith/youtube/blob/main/mPox_DL_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import and Set Up"
      ],
      "metadata": {
        "id": "1HP-nA_z8U99"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#packages needed\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import zipfile\n",
        "from sklearn.model_selection import train_test_split\n",
        "import albumentations as A\n"
      ],
      "metadata": {
        "id": "NCH_Gn2a2UGK"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZOHyxokpF4z",
        "outputId": "ad686361-91f5-4ff5-cf15-cb3f5969a52b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset extracted successfully.\n"
          ]
        }
      ],
      "source": [
        "# Extract the dataset\n",
        "zip_path = '/content/Monkeypox Skin Image Dataset.zip'  # Adjust this path if necessary\n",
        "extract_path = '/content'\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"Dataset extracted successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def create_dataset_df(data_dir):\n",
        "    image_paths = []\n",
        "    labels = []\n",
        "\n",
        "    for class_folder in os.listdir(data_dir):\n",
        "        class_path = os.path.join(data_dir, class_folder)\n",
        "        if os.path.isdir(class_path):\n",
        "            for image_file in os.listdir(class_path):\n",
        "                if image_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                    image_paths.append(os.path.join(class_folder, image_file))\n",
        "                    labels.append(class_folder)\n",
        "\n",
        "    df = pd.DataFrame({'image_path': image_paths, 'label': labels})\n",
        "    return df\n",
        "\n",
        "# Create the original dataset DataFrame\n",
        "extract_path = '/content/Monkeypox Skin Image Dataset'\n",
        "df_original = create_dataset_df(extract_path)\n",
        "\n",
        "print(\"Original dataset created.\")\n",
        "print(f\"Dataset size: {len(df_original)}\")\n",
        "print(\"\\nClass distribution:\")\n",
        "print(df_original['label'].value_counts())\n",
        "\n",
        "# Split the data into train and test sets\n",
        "train_df, test_df = train_test_split(df_original, test_size=0.2, stratify=df_original['label'], random_state=42)\n",
        "\n",
        "print(\"\\nTraining set size:\", len(train_df))\n",
        "print(\"Test set size:\", len(test_df))\n",
        "\n",
        "# Save the train and test DataFrames to CSV files\n",
        "train_df.to_csv('train_original.csv', index=False)\n",
        "test_df.to_csv('test_original.csv', index=False)\n",
        "\n",
        "print(\"\\nCSV files created.\")"
      ],
      "metadata": {
        "id": "d9gWzyxApHWg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5b2d86d-6d19-4729-dbd1-ec8da81b9f0f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original dataset created.\n",
            "Dataset size: 770\n",
            "\n",
            "Class distribution:\n",
            "label\n",
            "Normal        293\n",
            "Monkeypox     279\n",
            "Chickenpox    107\n",
            "Measles        91\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Training set size: 616\n",
            "Test set size: 154\n",
            "\n",
            "CSV files created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training on Original Dataset"
      ],
      "metadata": {
        "id": "khO1GOiL79Ne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Custom Model Definition\n",
        "class SEBlock(nn.Module):\n",
        "    def __init__(self, channel, reduction=16):\n",
        "        super(SEBlock, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channel, channel // reduction, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(channel // reduction, channel, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y.expand_as(x)\n",
        "\n",
        "class FPN(nn.Module):\n",
        "    def __init__(self, in_channels_list, out_channels):\n",
        "        super(FPN, self).__init__()\n",
        "        self.lateral_convs = nn.ModuleList([\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "            for in_channels in in_channels_list\n",
        "        ])\n",
        "        self.fpn_convs = nn.ModuleList([\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "            for _ in in_channels_list\n",
        "        ])\n",
        "\n",
        "    def forward(self, features):\n",
        "        last_feature = self.lateral_convs[-1](features[-1])\n",
        "        fpn_features = [self.fpn_convs[-1](last_feature)]\n",
        "\n",
        "        for feature, lateral_conv, fpn_conv in zip(\n",
        "            features[-2::-1], self.lateral_convs[-2::-1], self.fpn_convs[-2::-1]\n",
        "        ):\n",
        "            lateral = lateral_conv(feature)\n",
        "            feat_shape = lateral.shape[-2:]\n",
        "            top_down = F.interpolate(last_feature, size=feat_shape, mode='nearest')\n",
        "            last_feature = lateral + top_down\n",
        "            fpn_features.append(fpn_conv(last_feature))\n",
        "\n",
        "        return fpn_features[::-1]\n",
        "\n",
        "class CustomModel(nn.Module):\n",
        "    def __init__(self, num_classes=4):\n",
        "        super(CustomModel, self).__init__()\n",
        "        self.efficientnet = models.efficientnet_v2_s(pretrained=True)\n",
        "\n",
        "        # Get the number of features from the last layer of EfficientNetV2\n",
        "        num_features = self.efficientnet.classifier[1].in_features\n",
        "\n",
        "        # Remove the classifier\n",
        "        self.efficientnet = nn.Sequential(*list(self.efficientnet.children())[:-1])\n",
        "\n",
        "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc1 = nn.Linear(num_features, 512)\n",
        "        self.dropout1 = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.dropout2 = nn.Dropout(0.3)\n",
        "        self.fc3 = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.efficientnet(x)\n",
        "        x = self.gap(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the model\n",
        "model = CustomModel()\n",
        "print(f\"Model initialized with {sum(p.numel() for p in model.parameters())} parameters\")\n",
        "\n",
        "\n",
        "# Custom Dataset\n",
        "class MonkeypoxDataset(Dataset):\n",
        "    def __init__(self, csv_file, root_dir, transform=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.class_to_idx = {'Normal': 0, 'Monkeypox': 1, 'Chickenpox': 2, 'Measles': 3}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.root_dir, self.data.iloc[idx, 0])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label = self.class_to_idx[self.data.iloc[idx, 1]]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# Data transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "train_dataset = MonkeypoxDataset('train_original.csv', '/content/Monkeypox Skin Image Dataset', transform=transform)\n",
        "test_dataset = MonkeypoxDataset('test_original.csv', '/content/Monkeypox Skin Image Dataset', transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "# Initialize the model\n",
        "model = CustomModel()\n",
        "\n",
        "# Move model to device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Loss function (Focal Loss)\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=1, gamma=2):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss\n",
        "        return focal_loss.mean()\n",
        "\n",
        "criterion = FocalLoss()\n",
        "\n",
        "# Optimizer and learning rate scheduler\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
        "\n",
        "# Training function\n",
        "def train(model, train_loader, criterion, optimizer, scheduler, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        if i % 10 == 0:  # Print every 10 mini-batches\n",
        "            print(f'Batch {i}, Loss: {loss.item():.4f}')\n",
        "\n",
        "    scheduler.step()\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    return epoch_loss\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, test_loader, device):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
        "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "    # For AUC, we need probability scores\n",
        "    all_probs = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, _ in test_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            outputs = model(inputs)\n",
        "            probs = F.softmax(outputs, dim=1)\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "    all_probs = np.array(all_probs)\n",
        "    auc = roc_auc_score(all_labels, all_probs, multi_class='ovr', average='weighted')\n",
        "\n",
        "    return accuracy, precision, recall, f1, auc\n",
        "\n",
        "# Training loop\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "num_epochs = 100\n",
        "best_accuracy = 0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    train_loss = train(model, train_loader, criterion, optimizer, scheduler, device)\n",
        "    accuracy, precision, recall, f1, auc = evaluate(model, test_loader, device)\n",
        "\n",
        "    print(f\"Train Loss: {train_loss:.4f}\")\n",
        "    print(f\"Test Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}, AUC: {auc:.4f}\")\n",
        "\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "print(\"Training completed.\")\n",
        "\n",
        "# Load the best model and evaluate\n",
        "model.load_state_dict(torch.load('best_model.pth'))\n",
        "final_accuracy, final_precision, final_recall, final_f1, final_auc = evaluate(model, test_loader, device)\n",
        "\n",
        "print(\"Final Results on Original Dataset:\")\n",
        "print(f\"Accuracy: {final_accuracy:.4f}\")\n",
        "print(f\"Precision: {final_precision:.4f}\")\n",
        "print(f\"Recall: {final_recall:.4f}\")\n",
        "print(f\"F1-score: {final_f1:.4f}\")\n",
        "print(f\"AUC: {final_auc:.4f}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "-LYHiPM0pHbt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca954dfa-fde7-48d0-a469-11e8190b1feb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model initialized with 20965716 parameters\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_V2_S_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_V2_S_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "Batch 0, Loss: 0.7889\n",
            "Batch 10, Loss: 0.6377\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.4892\n",
            "Test Accuracy: 0.6494, Precision: 0.6016, Recall: 0.6494, F1: 0.6204, AUC: 0.8902\n",
            "Epoch 2/100\n",
            "Batch 0, Loss: 0.3949\n",
            "Batch 10, Loss: 0.2018\n",
            "Train Loss: 0.3488\n",
            "Test Accuracy: 0.7597, Precision: 0.8563, Recall: 0.7597, F1: 0.7730, AUC: 0.9299\n",
            "Epoch 3/100\n",
            "Batch 0, Loss: 0.1677\n",
            "Batch 10, Loss: 0.4253\n",
            "Train Loss: 0.2362\n",
            "Test Accuracy: 0.7792, Precision: 0.8257, Recall: 0.7792, F1: 0.7903, AUC: 0.9530\n",
            "Epoch 4/100\n",
            "Batch 0, Loss: 0.1241\n",
            "Batch 10, Loss: 0.2182\n",
            "Train Loss: 0.1441\n",
            "Test Accuracy: 0.8117, Precision: 0.8549, Recall: 0.8117, F1: 0.8206, AUC: 0.9590\n",
            "Epoch 5/100\n",
            "Batch 0, Loss: 0.1078\n",
            "Batch 10, Loss: 0.0418\n",
            "Train Loss: 0.1417\n",
            "Test Accuracy: 0.8831, Precision: 0.8820, Recall: 0.8831, F1: 0.8783, AUC: 0.9816\n",
            "Epoch 6/100\n",
            "Batch 0, Loss: 0.0376\n",
            "Batch 10, Loss: 0.0372\n",
            "Train Loss: 0.0635\n",
            "Test Accuracy: 0.9026, Precision: 0.9202, Recall: 0.9026, F1: 0.9038, AUC: 0.9796\n",
            "Epoch 7/100\n",
            "Batch 0, Loss: 0.0589\n",
            "Batch 10, Loss: 0.0352\n",
            "Train Loss: 0.0402\n",
            "Test Accuracy: 0.8766, Precision: 0.8732, Recall: 0.8766, F1: 0.8744, AUC: 0.9779\n",
            "Epoch 8/100\n",
            "Batch 0, Loss: 0.1613\n",
            "Batch 10, Loss: 0.0035\n",
            "Train Loss: 0.0325\n",
            "Test Accuracy: 0.9091, Precision: 0.9107, Recall: 0.9091, F1: 0.9089, AUC: 0.9736\n",
            "Epoch 9/100\n",
            "Batch 0, Loss: 0.0285\n",
            "Batch 10, Loss: 0.0354\n",
            "Train Loss: 0.0277\n",
            "Test Accuracy: 0.9221, Precision: 0.9272, Recall: 0.9221, F1: 0.9222, AUC: 0.9781\n",
            "Epoch 10/100\n",
            "Batch 0, Loss: 0.0175\n",
            "Batch 10, Loss: 0.0146\n",
            "Train Loss: 0.0075\n",
            "Test Accuracy: 0.9091, Precision: 0.9107, Recall: 0.9091, F1: 0.9089, AUC: 0.9793\n",
            "Epoch 11/100\n",
            "Batch 0, Loss: 0.0046\n",
            "Batch 10, Loss: 0.0049\n",
            "Train Loss: 0.0437\n",
            "Test Accuracy: 0.8117, Precision: 0.7950, Recall: 0.8117, F1: 0.7705, AUC: 0.9499\n",
            "Epoch 12/100\n",
            "Batch 0, Loss: 0.0851\n",
            "Batch 10, Loss: 0.1782\n",
            "Train Loss: 0.1304\n",
            "Test Accuracy: 0.7792, Precision: 0.8054, Recall: 0.7792, F1: 0.7725, AUC: 0.9346\n",
            "Epoch 13/100\n",
            "Batch 0, Loss: 0.2061\n",
            "Batch 10, Loss: 0.0609\n",
            "Train Loss: 0.1823\n",
            "Test Accuracy: 0.8052, Precision: 0.7945, Recall: 0.8052, F1: 0.7814, AUC: 0.9197\n",
            "Epoch 14/100\n",
            "Batch 0, Loss: 0.1332\n",
            "Batch 10, Loss: 0.0728\n",
            "Train Loss: 0.1261\n",
            "Test Accuracy: 0.8896, Precision: 0.8883, Recall: 0.8896, F1: 0.8855, AUC: 0.9734\n",
            "Epoch 15/100\n",
            "Batch 0, Loss: 0.0900\n",
            "Batch 10, Loss: 0.0264\n",
            "Train Loss: 0.1412\n",
            "Test Accuracy: 0.8377, Precision: 0.8601, Recall: 0.8377, F1: 0.8364, AUC: 0.9520\n",
            "Epoch 16/100\n",
            "Batch 0, Loss: 0.0847\n",
            "Batch 10, Loss: 0.0623\n",
            "Train Loss: 0.1073\n",
            "Test Accuracy: 0.8377, Precision: 0.8409, Recall: 0.8377, F1: 0.8314, AUC: 0.9668\n",
            "Epoch 17/100\n",
            "Batch 0, Loss: 0.1339\n",
            "Batch 10, Loss: 0.0604\n",
            "Train Loss: 0.0796\n",
            "Test Accuracy: 0.8896, Precision: 0.8949, Recall: 0.8896, F1: 0.8903, AUC: 0.9769\n",
            "Epoch 18/100\n",
            "Batch 0, Loss: 0.0156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Check out the model inference here : https://huggingface.co/spaces/ronithsharmila/PoxNet\n",
        "\n",
        "The model was deployed to Hugging Face Spaces using the instructions mentioned on this link:\n",
        "\n"
      ],
      "metadata": {
        "id": "FW7dPjU_bMzO"
      }
    }
  ]
}