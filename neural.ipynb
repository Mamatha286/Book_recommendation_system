{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps involved in building this neural network\n",
    "\n",
    "1. Importing the packages\n",
    "2.Load and preprocess the data\n",
    "3.Initialize the parameters\n",
    "4.Activation setup and Forward Propogation  \n",
    "5.Compute Loss\n",
    "6.Backward propogation\n",
    "7.Update Parameters\n",
    "8.TRAIN THE NETWORK\n",
    "9. Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 4) (120, 3)\n",
      "(30, 4) (30, 3)\n"
     ]
    }
   ],
   "source": [
    "#2.Load and preprocess the data\n",
    "\n",
    "iris=load_iris()\n",
    "x=iris.data\n",
    "y=iris.target\n",
    "\n",
    "encoder=OneHotEncoder(sparse_output=False)\n",
    "y=encoder.fit_transform(y.reshape(-1,1))\n",
    "\n",
    "#normalizing the feartures\n",
    "\n",
    "scaler=StandardScaler()\n",
    "X=scaler.fit_transform(X)\n",
    "\n",
    "#split the dataset into train and test\n",
    "\n",
    "X_train, X_test, y_train,y_test=train_test_split(X,y,test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "print (X_train.shape, y_train.shape)\n",
    "print (X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.Initialize the parameters\n",
    "\n",
    "\n",
    "def initialize_parameters(input_size,hidden_size,output_size):\n",
    "    np.random.seed(42)\n",
    "    W1=np.random.randn(input_size,hidden_size)*0.01\n",
    "    b1=np.zeros((1,hidden_size))\n",
    "    W2=np.random.randn(hidden_size,output_size)*0.01\n",
    "    b2=np.zeros((1,output_size))\n",
    "\n",
    "    return W1,b1,W2,b2\n",
    "\n",
    "\n",
    "\n",
    "input_size=4 #basically the number of features\n",
    "hidden_size=10 \n",
    "output_size=3 #Number of classes\n",
    "\n",
    "\n",
    "W1,b1,W2,b2=initialize_parameters(input_size,hidden_size,output_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 Activation and Forward Propogation\n",
    "\n",
    "#using RELU and SOFTMAX activations\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "def softmax(Z):\n",
    "    expZ=np.exp(Z-np.max(Z, axis=1, keepdims=True))\n",
    "    return expZ / np.sum(expZ, axis=1, keepdims=True)\n",
    "\n",
    "def forward_propagation(X, W1, b1, W2, b2):\n",
    "    Z1=np.dot(X, W1) + b1 #Linear function \n",
    "    A1=relu(Z1) #Activation makes in non-linear\n",
    "    Z2=np.dot(A1, W2) + b2\n",
    "    A2=relu(Z2)\n",
    "    return Z1, A1, Z2, A2 \n",
    "\n",
    "Z1, A1, Z2, A2 =forward_propagation(X_train, W1,b1, W2,b2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5 Compute Loss\n",
    "\n",
    "def compute_loss(Y, A2):\n",
    "    m=Y.shape[0]\n",
    "    loss= -np.sum(Y*np.log(A2 + 1e-8)) / m\n",
    "    return loss\n",
    "\n",
    "#We are defining the cross-entropy loss function and computing the intial loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. Backward Propogation\n",
    "\n",
    "def relu_derivative(Z):\n",
    "    return Z > 0\n",
    "\n",
    "def backward_propagation(X, Y, Z1, A1, Z2, A2,W1,W2):\n",
    "    m= X.shape[0]\n",
    "    dZ2=A2-Y\n",
    "    dW2=np.dot(A1.T, dZ2) / m\n",
    "    db2=np.sum(dZ2, axis=0, keepdims=True) / m\n",
    "\n",
    "\n",
    "    dA1=np.dot(dZ2, W2.T)\n",
    "    dZ1=dA1 * relu_derivative(Z1)\n",
    "    dW1=np.dot(X.T, dZ1)\n",
    "    db1=np.sum(dZ1, axis=0, keepdims=True) / m\n",
    "\n",
    "\n",
    "    return dW1, db1, dW2, db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7 Updating the parameters\n",
    "\n",
    "def update_parameters(W1,b1,W2,b2, dW1, db1,dW2,db2, learning_rate):\n",
    "    W1-=learning_rate*dW1\n",
    "    b1-=learning_rate*db1\n",
    "    W2-=learning_rate*dW2\n",
    "    b2-=learning_rate*db2\n",
    "    return W1, b1, W2, b2    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch0, Loss:5.615465835898257\n",
      "Epoch100, Loss:0.5703094200003777\n",
      "Epoch200, Loss:0.37591908539308017\n",
      "Epoch300, Loss:0.29581992842815524\n",
      "Epoch400, Loss:0.2618316752396724\n",
      "Epoch500, Loss:0.23778498598026848\n",
      "Epoch600, Loss:0.21820287313181386\n",
      "Epoch700, Loss:0.20188303570763277\n",
      "Epoch800, Loss:0.18791136000820446\n",
      "Epoch900, Loss:0.17587305671508377\n"
     ]
    }
   ],
   "source": [
    "#8. Training the network\n",
    "\n",
    "def train(X_train, y_train, W1, b1, W2, b2, epochs, learning_rate):\n",
    "    for epoch in range(epochs):\n",
    "        Z1, A1,Z2, A2 = forward_propagation(X_train, W1, b1, W2,b2)\n",
    "        loss=compute_loss(y_train,A2)\n",
    "        dW1, db1, dW2,db2=backward_propagation(X_train,y_train,Z1, A1, Z2, A2, W1, W2)\n",
    "        W1,b1,W2,b2=update_parameters(W1,b1,W2,b2,dW1, db1,dW2, db2,learning_rate)\n",
    "\n",
    "        if epoch % 100==0:\n",
    "            print (f\"Epoch{epoch}, Loss:{loss}\")\n",
    "\n",
    "\n",
    "    return W1, b1, W2,b2\n",
    "\n",
    "\n",
    "epochs=1000\n",
    "learning_rate=0.01\n",
    "\n",
    "W1,b1,W2,b2=train(X_train,y_train,W1,b1,W2,b2,epochs,learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "#9 Make predictions\n",
    "\n",
    "def predict(X,W1,b1,W2,b2):\n",
    "    _,_,_, A2= forward_propagation(X,W1,b1,W2,b2)\n",
    "    return np.argmax(A2, axis=1)\n",
    "\n",
    "\n",
    "preiddctions=predict(X_test, W1, b1, W2, b2)\n",
    "accuracy=np.mean(preiddctions==np.argmax(y_test,axis=1))\n",
    "print(f\"accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
